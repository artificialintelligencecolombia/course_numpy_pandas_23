Not exactly, but I can see how one might arrive at that interpretation. Let's clarify:

Both variance and standard deviation deal with the dispersion of data points in relation to the mean of the dataset. The key distinction is in how they quantify this dispersion.

### Variance:
- **Variance** is indeed an "average." Specifically, it's the average of the squared differences from the mean for all data points in the dataset. 
- This means you take each data point, find its difference from the mean, square that difference, and then average all these squared differences.

### Standard Deviation:
- **Standard Deviation** is not about individual data points per se, but it does provide a more intuitive sense of the "typical" or "average" distance from the mean for the data points in the dataset.
- It's essentially the square root of the variance, so it's like asking, "On average, how far are individual data points from the mean?" without squaring the differences.

#### Analogy:

Imagine you're comparing the spread of test scores for two classes:

1. **Class A** has a **variance** of 64 in test scores. This means, on average, the squared differences of scores from the mean score is 64.
   
2. **Class B** has a **standard deviation** of 8 in test scores. This gives you an intuitive sense that, on average, students' scores in this class deviate by 8 points from the mean score.

It turns out, the standard deviation of Class A would also be 8 because the square root of 64 is 8. So, while variance gives you a sense of the "average squared difference from the mean," the standard deviation gives you a sense of the "average difference from the mean" in original units, without the squaring.

In summary: Both variance and standard deviation provide measures of the spread of data points in relation to the mean. Variance calculates the average of the squared differences, while standard deviation offers a more direct and often more interpretable measure of the typical distance of data points from the mean.